{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "r8VjAuKXgGFS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_analysis(X, y, w, b):\n",
        "\n",
        "  # X = np.asarray(X)\n",
        "  # y = np.asarray(y)\n",
        "  # w = np.asarray(w)\n",
        "\n",
        "  decision_val = np.dot(X, w) + b\n",
        "\n",
        "  sv = np.where(np.abs(decision_val) <= 1 + 1e-5)[0]\n",
        "\n",
        "  posi_sv = sv[y[sv] == 1]\n",
        "  nega_sv = sv[y[sv] == -1]\n",
        "\n",
        "  margin = 2 / np.linalg.norm(w)\n",
        "\n",
        "  posi_margin = (1-b)/w[0] if w[0] != 0 else None\n",
        "  nega_margin = (-1-b)/w[0] if w[0] != 0 else None\n",
        "\n",
        "  return {\n",
        "        \"positive_sv\": posi_sv,\n",
        "        \"negative_sv\": nega_sv,\n",
        "        \"margin\": margin,\n",
        "        \"positive_margin\": posi_margin,\n",
        "        \"negative_margin\": nega_margin\n",
        "  }\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "KhMwrqjDf5Kp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_bayes(X, y):\n",
        "\n",
        "    # X = np.asarray(X)\n",
        "    # y = np.asarray(y)\n",
        "\n",
        "    # Initialize the result dictionary\n",
        "    # conditional_probs = {}\n",
        "    # in condition y==0 and y==1\n",
        "    conditional_probs = {0: {}, 1: {}}\n",
        "\n",
        "    for cls in [0,1]:\n",
        "      X_cls = X[y == cls]\n",
        "\n",
        "      for feature_idx in range(X.shape[1]):\n",
        "        feature_val = X_cls[:, feature_idx]\n",
        "        unique_val, counts = np.unique(feature_val, return_counts=True)\n",
        "        feature_probs = {val: count/len(feature_val) for val, count in zip(unique_val, counts)}\n",
        "        conditional_probs[cls][feature_idx] = feature_probs\n",
        "\n",
        "    return conditional_probs"
      ],
      "metadata": {
        "id": "EGGpjwHLmSm8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manhattan_distance(p, q):\n",
        "  distances = np.sum(np.abs(p-q))\n",
        "  return distances"
      ],
      "metadata": {
        "id": "us39IWmBuE0f"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_distance(p, q, weights):\n",
        "  distances = np.sqrt(np.sum(weights * np.abs(p-q)**2))\n",
        "  return distances"
      ],
      "metadata": {
        "id": "ZaVHY5-9uVmj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance(p, q):\n",
        "  distances = np.sqrt(np.sum(p-q)**2)\n",
        "  return distances"
      ],
      "metadata": {
        "id": "ZFEhFEAAuWho"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_impurity(y):\n",
        "\n",
        "  #y = np.asarray(y)\n",
        "\n",
        "  class_count = np.bincount(y)\n",
        "  proportions = class_count / len(y)\n",
        "  gini = 1 - sum(proportions ** 2)\n",
        "\n",
        "  return gini"
      ],
      "metadata": {
        "id": "Vb7dMic_xWNY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(y):\n",
        "\n",
        "    # y = np.asarray(y)\n",
        "    class_counts = np.bincount(y)\n",
        "    probabilities = class_counts / len(y)\n",
        "    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
        "    return entropy_value\n",
        "\n",
        "def information_gain(X, y, feature_idx):\n",
        "\n",
        "    original_entropy = entropy(y)\n",
        "    feature_val, counts = np.unique(X[:, feature_idx], return_counts=True)\n",
        "    weighted_entropy = 0\n",
        "    for val, count in zip(feature_val, counts):\n",
        "      subset_y = y[X[:, feature_idx] == val]\n",
        "      weighted_entropy += (len(subset_y) / len(y)) * entropy(subset_y)\n",
        "      gain = original_entropy - weighted_entropy\n",
        "\n",
        "    return gain"
      ],
      "metadata": {
        "id": "-PFtalqmykFj"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}